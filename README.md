# â„ï¸ Frozen Lake SARSA Agent - Reinforcement Learning Project â„ï¸

This project implements the SARSA (State-Action-Reward-State-Action) reinforcement learning algorithm to solve the FrozenLake environment from Gymnasium. The agent learns to navigate a slippery grid world to reach a goal while avoiding holes. ðŸ§Šâž¡ï¸ðŸŽ¯

## ðŸŒ Environment Overview
The FrozenLake environment features:
- 4Ã—4 grid world (16 states) ðŸ—ºï¸
- 4 possible actions (â†, â†“, â†’, â†‘) ðŸŽ®
- Start state (S) and goal state (G) ðŸ
- Frozen tiles (safe) and holes (H) that end the episode ðŸ•³ï¸
- Two modes:
  - **Slippery**: Stochastic transitions (33% chance of moving sideways) ðŸ§Š
  - **Non-slippery**: Deterministic transitions ðŸ§Šâœ…

![Frozen Lake Environment](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*XskZ2mrp_3QHTt2VHh1lPA.png)

## âš™ï¸ Implementation Details
- **Algorithm**: SARSA (on-policy TD control) ðŸ§ 
- **Action Selection**: Îµ-greedy policy with decay ðŸ”
- **Hyperparameters**:
  - Learning rate (Î±) = 0.1 ðŸ“ˆ
  - Discount factor (Î³) = 0.95 ðŸ’°
  - Initial Îµ = 0.9 ðŸŽ²
  - Îµ decay = 0.995 â¬
  - Minimum Îµ = 0.01 ðŸŽ¯
  - Episodes = 2000 (optimal for slippery) / 5000 ðŸ”

## ðŸ“Š Performance Observations

### ðŸ§Š Slippery Conditions (Stochastic Environment)
| Configuration       | Episodes | Îµ Decay | Avg. Success Rate | Performance |
|---------------------|----------|---------|-------------------|-------------|
| **Optimal** âœ…       | 2000     | Yes     | 58%               | Best        |
| Suboptimal âŒ       | 5000     | Yes     | Reduced           | Worse       |
| Without Îµ Decay     | 2000     | No      | Moderate          | Medium      |

**Key Findings**:
- ðŸ† Best performance with 2000 episodes and Îµ decay
- âš ï¸ 5000 episodes with Îµ decay performed worse (over-training?)
- ðŸ” Îµ decay is crucial for balancing exploration/exploitation
- ðŸ§ª Stochastic transitions make learning challenging

### ðŸ§Š Non-slippery Conditions (Deterministic Environment)
| Configuration       | Episodes | Îµ Decay | Performance       |
|---------------------|----------|---------|-------------------|
| All tested          | 2000     | Yes     | Consistently High âœ… |
| All tested          | 5000     | No      | Consistently High âœ… |

**Key Findings**:
- âœ… Agent achieves near-perfect performance regardless of configuration
- ðŸ§  Environment simplicity makes learning straightforward
- âš–ï¸ Îµ decay has minimal impact due to deterministic transitions


## ðŸ’¡ Key Insights
1. **Environment Matters**:
   - ðŸ§Š Slippery conditions require careful hyperparameter tuning
   - ðŸ§Š Non-slippery conditions are solved easily with any configuration

2. **Îµ Decay Strategy**:
   - ðŸ“‰ Crucial for stochastic environments
   - âš–ï¸ Balances exploration vs exploitation
   - â±ï¸ Requires proper tuning of decay rate and minimum Îµ

3. **Training Duration**:
   - â³ More episodes â‰  better performance
   - ðŸ›‘ Over-training can lead to policy degradation
   - ðŸ” Optimal episode count differs between environments

4. **Algorithm Behavior**:
   - ðŸ”„ SARSA performs well for on-policy learning
   - ðŸŽ¯ Q-learning might yield better results in deterministic cases
   - ðŸ§® Expected SARSA could improve slippery condition performance

## ðŸ”® Future Improvements
- ðŸŽ¯ Implement reward shaping for more efficient learning
- ðŸ§  Add neural network function approximation (Deep SARSA)
- ðŸ“Š Develop policy visualization tools
- âš–ï¸ Compare performance with Q-learning and Expected SARSA
- ðŸ” Experiment with different exploration strategies
- ðŸ“ˆ Add training progress tracking and early stopping
- ðŸ§ª Test on larger 8Ã—8 FrozenLake maps

```mermaid
graph LR
A[Start] --> B[Initialize Q-table]
B --> C[Set hyperparameters]
C --> D[Training Loop]
D --> E[Select action with Îµ-greedy]
E --> F[Take action, observe next state]
F --> G[Select next action]
G --> H[Update Q-value]
H --> I{Episode done?}
I -- No --> F
I -- Yes --> J[Decay Îµ]
J --> K{All episodes done?}
K -- No --> D
K -- Yes --> L[Evaluate Policy]
```
